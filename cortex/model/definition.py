from enum import Enum
from typing import Any, Dict, List, Optional
from typing_extensions import TypedDict

from pydantic import BaseModel, Field


class MessageRole(Enum):
    USER = "user"
    HUMAN = "human"
    ASSISTANT = "assistant"
    SYSTEM = "system"
    TOOL_DESC = "tool-description"
    TOOL_RESPONSE = "tool-response"
    TOOL = "tool"


class ContentBlockType(Enum):
    TEXT = "text"
    THINK = "thinking"
    REDACTED_THINK = "redacted_thinking"
    TOOLCALL = "toolcall"
    TOOLUSE = "tool_use"
    TOOLRESULT = "tool_result"
    IMAGE = "image"
    IMAGE_URL = "image_url"
    VIDEO = "video"
    VIDEO_URL = "video_url"
    AUDIO = "input_audio"
    AUDIO_URL = "audio_url"
    DOCURL = "doc_url"


class ModelParams(BaseModel):
    name: str
    response_format: BaseModel | None = None
    toolcall_parser_version: str | None = None
    parallel_tool_calls: bool = True
    infer_kwargs: dict = {}
    user_role: str = MessageRole.USER.value
    tool_role: str = MessageRole.TOOL.value
    explicit_model_family_value: str | None = None
    explicit_api_base: str | None = None
    explicit_api_key: str | None = None


class Function(BaseModel):
    arguments: str | None = None
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str | None = None
    """The name of the function to call."""


class ChatToolCall(BaseModel):
    index: int | None = None
    """The index of the tool call."""

    id: str | None = None
    """The ID of the tool call."""

    function: Function
    """The function that the model called."""

    type: str | None = "function"
    """The type of the tool. Currently, only `function` is supported."""


class ExtraInfo(TypedDict, total=False):
    """Extra information for ChatMessage."""

    cache_msg_id: str
    """The cache message ID.

    Try best to leverage the cache functionality of the backend model service.
    But it's not guaranteed to be successful.
    Currently only Anthropic model of models-proxy backend supports this functionality.
    """

    usage: Dict[str, Any]
    """The usage information of the model. """

    finish_reason: str
    """The finish reason of the model.

    For OpenAI model, it's called "finish_reason".
    For Anthropic model, it's called "stop_reason".
    Here we use the unified name "finish_reason" for all cases.
    """


class ChatMessage(BaseModel):
    id: Optional[str] = None
    role: str | None = None
    content: str | List[Dict] | None = None
    tool_call_id: Optional[str] = None
    tool_calls: Optional[List[ChatToolCall]] = Field(default_factory=list)

    # for train use
    extra_info: Optional[ExtraInfo] = None

    @classmethod
    def from_dict(cls, data: dict) -> "ChatMessage":
        return cls(**data)

    def to_dict(self) -> dict:
        return self.model_dump()
